{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f07c8e2",
   "metadata": {},
   "source": [
    "# Oscar Edward Guijaya - 2301981975\n",
    "\n",
    "# Deteksi Kanker Payudara menggunakan Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c071eba8",
   "metadata": {},
   "source": [
    "# Import Library awal yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6584e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import random\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2eb199",
   "metadata": {},
   "source": [
    "# Mempersiapkan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310e2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPaths = list(paths.list_images(\"datasets/original\"))\n",
    "random.seed(42)\n",
    "random.shuffle(imgPaths)\n",
    "\n",
    "i = int(len(imgPaths) * 0.8)\n",
    "trainPaths = imgPaths[:i]\n",
    "testPaths = imgPaths[i:]\n",
    "\n",
    "\n",
    "i = int(len(trainPaths) * 0.1)\n",
    "valPaths = trainPaths[:i]\n",
    "trainPaths = trainPaths[i:]\n",
    "\n",
    "datasets = [\n",
    "    (\"training\", trainPaths, \"datasets/splitted/training\"),\n",
    "    (\"validation\", valPaths, \"datasets/splitted/validation\"),\n",
    "    (\"testing\", testPaths, \"datasets/splitted/testing\")\n",
    "]\n",
    "\n",
    "# loop over the datasets\n",
    "for (dType, imagePaths, baseOutput) in datasets:\n",
    "    if not os.path.exists(baseOutput):\n",
    "        os.makedirs(baseOutput)\n",
    "        \n",
    "    for inputPath in imagePaths:\n",
    "        filename = inputPath.split(os.path.sep)[-1]\n",
    "        label = filename[-5:-4]\n",
    "        labelPath = os.path.sep.join([baseOutput, label])\n",
    "        if not os.path.exists(labelPath):\n",
    "            os.makedirs(labelPath)\n",
    "        p = os.path.sep.join([labelPath, filename])\n",
    "        shutil.copy2(inputPath, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11358e",
   "metadata": {},
   "source": [
    "# Import Library yang diperlukan untuk model CancerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ccc62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import SeparableConv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a27953d",
   "metadata": {},
   "source": [
    "# Membuat static method untuk model CancerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8257917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        # if we are using \"channels first\", update the input shape\n",
    "        # and channels dimension\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            chanDim = 1\n",
    "            \n",
    "        # CONV => RELU => POOL\n",
    "        model.add(SeparableConv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # (CONV => RELU => POOL) * 2\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(64, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # (CONV => RELU => POOL) * 3\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(SeparableConv2D(128, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        \n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d16126",
   "metadata": {},
   "source": [
    "# Import Library yang diperlukan untuk proses Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa5427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39929b",
   "metadata": {},
   "source": [
    "# Mendefinisikan parameter untuk training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5410b24",
   "metadata": {},
   "source": [
    "### Menentukan jumlah epoch, menghitung jumlah dataset dan menghitung class weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4add0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 40\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "trainPaths = list(paths.list_images(\"datasets/splitted/training\"))\n",
    "totalTrain = len(trainPaths)\n",
    "totalVal = len(list(paths.list_images(\"datasets/splitted/validation\")))\n",
    "totalTest = len(list(paths.list_images(\"datasets/splitted/testing\")))\n",
    "\n",
    "trainLabels = [int(p.split(os.path.sep)[-2]) for p in trainPaths]\n",
    "trainLabels = to_categorical(trainLabels)\n",
    "classTotals = trainLabels.sum(axis=0)\n",
    "classWeight = dict()\n",
    "\n",
    "for i in range(0, len(classTotals)):\n",
    "    classWeight[i] = classTotals.max() / classTotals[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df09c43",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "135f89ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAug = ImageDataGenerator(\n",
    "    rescale=1 / 255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.05,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "valAug = ImageDataGenerator(rescale=1 / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d08c1a",
   "metadata": {},
   "source": [
    "# Membuat Generator training dari object Data Augmentation yang sudah dibuat sebelumnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39376091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199818 images belonging to 2 classes.\n",
      "Found 22201 images belonging to 2 classes.\n",
      "Found 55505 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trainGen = trainAug.flow_from_directory(\n",
    "    \"datasets/splitted/training\",\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=True,\n",
    "    batch_size=BS)\n",
    "\n",
    "valGen = valAug.flow_from_directory(\n",
    "    \"datasets/splitted/validation\",\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)\n",
    "\n",
    "testGen = valAug.flow_from_directory(\n",
    "    \"datasets/splitted/testing\",\n",
    "    class_mode=\"categorical\",\n",
    "    target_size=(48, 48),\n",
    "    color_mode=\"rgb\",\n",
    "    shuffle=False,\n",
    "    batch_size=BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfa1f8",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f619eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adagrad.py:77: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adagrad, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "6244/6244 [==============================] - 1572s 251ms/step - loss: 0.6170 - accuracy: 0.8166 - val_loss: 0.6673 - val_accuracy: 0.7061\n",
      "Epoch 2/40\n",
      "6244/6244 [==============================] - 940s 150ms/step - loss: 0.5632 - accuracy: 0.8308 - val_loss: 0.6349 - val_accuracy: 0.7322\n",
      "Epoch 3/40\n",
      "6244/6244 [==============================] - 833s 133ms/step - loss: 0.5527 - accuracy: 0.8339 - val_loss: 0.5935 - val_accuracy: 0.7487\n",
      "Epoch 4/40\n",
      "6244/6244 [==============================] - 860s 138ms/step - loss: 0.5498 - accuracy: 0.8346 - val_loss: 0.5986 - val_accuracy: 0.7428\n",
      "Epoch 5/40\n",
      "6244/6244 [==============================] - 847s 136ms/step - loss: 0.5448 - accuracy: 0.8356 - val_loss: 0.5675 - val_accuracy: 0.7584\n",
      "Epoch 6/40\n",
      "6244/6244 [==============================] - 868s 139ms/step - loss: 0.5437 - accuracy: 0.8357 - val_loss: 0.5659 - val_accuracy: 0.7561\n",
      "Epoch 7/40\n",
      "6244/6244 [==============================] - 865s 139ms/step - loss: 0.5391 - accuracy: 0.8371 - val_loss: 0.5719 - val_accuracy: 0.7605\n",
      "Epoch 8/40\n",
      "6244/6244 [==============================] - 845s 135ms/step - loss: 0.5410 - accuracy: 0.8371 - val_loss: 0.5957 - val_accuracy: 0.7439\n",
      "Epoch 9/40\n",
      "6244/6244 [==============================] - 837s 134ms/step - loss: 0.5390 - accuracy: 0.8374 - val_loss: 0.5565 - val_accuracy: 0.7656\n",
      "Epoch 10/40\n",
      "6244/6244 [==============================] - 771s 123ms/step - loss: 0.5380 - accuracy: 0.8384 - val_loss: 0.5557 - val_accuracy: 0.7665\n",
      "Epoch 11/40\n",
      "6244/6244 [==============================] - 753s 121ms/step - loss: 0.5371 - accuracy: 0.8388 - val_loss: 0.5583 - val_accuracy: 0.7656\n",
      "Epoch 12/40\n",
      "6244/6244 [==============================] - 742s 119ms/step - loss: 0.5381 - accuracy: 0.8372 - val_loss: 0.5619 - val_accuracy: 0.7632\n",
      "Epoch 13/40\n",
      "6244/6244 [==============================] - 888s 142ms/step - loss: 0.5395 - accuracy: 0.8369 - val_loss: 0.5397 - val_accuracy: 0.7754\n",
      "Epoch 14/40\n",
      "6244/6244 [==============================] - 952s 152ms/step - loss: 0.5366 - accuracy: 0.8377 - val_loss: 0.5588 - val_accuracy: 0.7668\n",
      "Epoch 15/40\n",
      "6244/6244 [==============================] - 956s 153ms/step - loss: 0.5353 - accuracy: 0.8383 - val_loss: 0.5533 - val_accuracy: 0.7672\n",
      "Epoch 16/40\n",
      "6244/6244 [==============================] - 967s 155ms/step - loss: 0.5352 - accuracy: 0.8383 - val_loss: 0.5548 - val_accuracy: 0.7678\n",
      "Epoch 17/40\n",
      "6244/6244 [==============================] - 954s 153ms/step - loss: 0.5348 - accuracy: 0.8383 - val_loss: 0.5406 - val_accuracy: 0.7730\n",
      "Epoch 18/40\n",
      "6244/6244 [==============================] - 961s 154ms/step - loss: 0.5343 - accuracy: 0.8382 - val_loss: 0.5472 - val_accuracy: 0.7702\n",
      "Epoch 19/40\n",
      "6244/6244 [==============================] - 957s 153ms/step - loss: 0.5353 - accuracy: 0.8387 - val_loss: 0.5439 - val_accuracy: 0.7727\n",
      "Epoch 20/40\n",
      "6244/6244 [==============================] - 964s 154ms/step - loss: 0.5349 - accuracy: 0.8383 - val_loss: 0.5380 - val_accuracy: 0.7738\n",
      "Epoch 21/40\n",
      "6244/6244 [==============================] - 952s 152ms/step - loss: 0.5348 - accuracy: 0.8388 - val_loss: 0.5320 - val_accuracy: 0.7767\n",
      "Epoch 22/40\n",
      "6244/6244 [==============================] - 951s 152ms/step - loss: 0.5343 - accuracy: 0.8387 - val_loss: 0.5245 - val_accuracy: 0.7816\n",
      "Epoch 23/40\n",
      "6244/6244 [==============================] - 965s 155ms/step - loss: 0.5356 - accuracy: 0.8391 - val_loss: 0.5440 - val_accuracy: 0.7720\n",
      "Epoch 24/40\n",
      "6244/6244 [==============================] - 949s 152ms/step - loss: 0.5339 - accuracy: 0.8389 - val_loss: 0.5434 - val_accuracy: 0.7732\n",
      "Epoch 25/40\n",
      "6244/6244 [==============================] - 958s 153ms/step - loss: 0.5353 - accuracy: 0.8387 - val_loss: 0.5446 - val_accuracy: 0.7722\n",
      "Epoch 26/40\n",
      "6244/6244 [==============================] - 956s 153ms/step - loss: 0.5333 - accuracy: 0.8390 - val_loss: 0.5248 - val_accuracy: 0.7812\n",
      "Epoch 27/40\n",
      "6244/6244 [==============================] - 957s 153ms/step - loss: 0.5339 - accuracy: 0.8386 - val_loss: 0.5363 - val_accuracy: 0.7757\n",
      "Epoch 28/40\n",
      "6244/6244 [==============================] - 961s 154ms/step - loss: 0.5320 - accuracy: 0.8388 - val_loss: 0.5170 - val_accuracy: 0.7840\n",
      "Epoch 29/40\n",
      "6244/6244 [==============================] - 951s 152ms/step - loss: 0.5323 - accuracy: 0.8390 - val_loss: 0.5240 - val_accuracy: 0.7822\n",
      "Epoch 30/40\n",
      "6244/6244 [==============================] - 956s 153ms/step - loss: 0.5323 - accuracy: 0.8396 - val_loss: 0.5282 - val_accuracy: 0.7789\n",
      "Epoch 31/40\n",
      "6244/6244 [==============================] - 1007s 161ms/step - loss: 0.5334 - accuracy: 0.8391 - val_loss: 0.5360 - val_accuracy: 0.7753\n",
      "Epoch 32/40\n",
      "6244/6244 [==============================] - 1113s 178ms/step - loss: 0.5329 - accuracy: 0.8394 - val_loss: 0.5353 - val_accuracy: 0.7768\n",
      "Epoch 33/40\n",
      "6244/6244 [==============================] - 962s 154ms/step - loss: 0.5332 - accuracy: 0.8392 - val_loss: 0.5188 - val_accuracy: 0.7833\n",
      "Epoch 34/40\n",
      "6244/6244 [==============================] - 954s 153ms/step - loss: 0.5337 - accuracy: 0.8388 - val_loss: 0.5305 - val_accuracy: 0.7776\n",
      "Epoch 35/40\n",
      "6244/6244 [==============================] - 948s 152ms/step - loss: 0.5319 - accuracy: 0.8392 - val_loss: 0.5318 - val_accuracy: 0.7768\n",
      "Epoch 36/40\n",
      "6244/6244 [==============================] - 959s 154ms/step - loss: 0.5320 - accuracy: 0.8399 - val_loss: 0.5234 - val_accuracy: 0.7817\n",
      "Epoch 37/40\n",
      "6244/6244 [==============================] - 955s 153ms/step - loss: 0.5315 - accuracy: 0.8399 - val_loss: 0.5271 - val_accuracy: 0.7798\n",
      "Epoch 38/40\n",
      "6244/6244 [==============================] - 961s 154ms/step - loss: 0.5326 - accuracy: 0.8390 - val_loss: 0.5342 - val_accuracy: 0.7770\n",
      "Epoch 39/40\n",
      "6244/6244 [==============================] - 1016s 163ms/step - loss: 0.5328 - accuracy: 0.8392 - val_loss: 0.5226 - val_accuracy: 0.7827\n",
      "Epoch 40/40\n",
      "6244/6244 [==============================] - 1092s 175ms/step - loss: 0.5320 - accuracy: 0.8392 - val_loss: 0.5228 - val_accuracy: 0.7816\n"
     ]
    }
   ],
   "source": [
    "model = CancerNet.build(width=48, height=48, depth=3,classes=2)\n",
    "opt = Adagrad(lr=INIT_LR, decay=INIT_LR / NUM_EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit(\n",
    "    x=trainGen,\n",
    "    steps_per_epoch=totalTrain // BS,\n",
    "    validation_data=valGen,\n",
    "    validation_steps=totalVal // BS,\n",
    "    class_weight=classWeight,\n",
    "    epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabe2c6",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5e39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1735/1735 [==============================] - 345s 198ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.72      0.83     39793\n",
      "           1       0.57      0.93      0.71     15712\n",
      "\n",
      "    accuracy                           0.78     55505\n",
      "   macro avg       0.77      0.82      0.77     55505\n",
      "weighted avg       0.85      0.78      0.79     55505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testGen.reset()\n",
    "predIdxs = model.predict(x=testGen, steps=(totalTest // BS) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(testGen.classes, predIdxs,target_names=testGen.class_indices.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396f5bc",
   "metadata": {},
   "source": [
    "## Evaluasi menggunakan confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8edd5d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[28800 10993]\n",
      " [ 1163 14549]]\n",
      "acc: 0.7810\n",
      "sensitivity: 0.7237\n",
      "specificity: 0.9260\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(testGen.classes, predIdxs)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
